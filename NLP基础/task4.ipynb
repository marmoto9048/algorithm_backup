{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1，朴素贝叶斯\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 寻找文本的某些特征，然后根据这些特征将文本归为某个类。\n",
    "像naive bayes这样的生成分类器构建了每个类的模型。通过观察，他们返回最有可能生成观察结果的类。\n",
    "\n",
    "像logistic回归这样的区分分类器可以从输入中学习哪些特征对于区分不同的可能类最有用。\n",
    "\n",
    "优点：在数据较少的情况下仍然有效，可以处理多类别问题。\n",
    "缺点：对于输入数据的准备方式较为敏感。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='1.png'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 朴素贝叶斯的原理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练朴素贝叶斯的过程其实就是计算先验概率和似然函数的过程。\n",
    "\n",
    "①先验概率P(c)的计算\n",
    "\n",
    "②似然函数P(wi|c)的计算\n",
    "\n",
    "由于是用词袋模型表示一篇文档d，对于文档d中的每个单词wi，\n",
    "找到训练数据集中所有类别为c的文档，\n",
    "数一数单词wi在这些文档（类别为c）中出现的次数：count(wi,c)\n",
    "\n",
    "然后，再数一数训练数据集中类别为c的文档一共有多少个单词。计算二者之间的比值，\n",
    "就是似然函数的值。似然函数计算公式如下：\n",
    "\n",
    "\n",
    "其中V，就是词库。（有些单词在词库中，但是不属于类别C，那么 count(w,c)=0）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='2.png'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='2.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用朴素贝叶斯模型进行文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='3.png'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='3.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 载入数据集：6条文本及它们各自的类别，这6条文本作为训练集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "\n",
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 创建词汇表：利用集合结构内元素的唯一性，创建一个包含所有词汇的词表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dataSet):\n",
    "    vocabSet = set([])  #create empty set\n",
    "    for document in dataSet:\n",
    "        vocabSet = vocabSet | set(document) #union of the two sets\n",
    "    return list(vocabSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 把输入文本根据词表转化为计算机可处理的01向量形式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        else: print(\"the word: %s is not in my Vocabulary!\" % word)\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 训练模型：在训练样本中计算先验概率 p(Ci) 和 条件概率 p(x,y | Ci)，本实例有0和1两个类别，所以返回p(x,y | 0)，p(x,y | 1)和p(Ci)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNB0(trainMatrix,trainCategory):\n",
    "    numTrainDocs = len(trainMatrix)\n",
    "    numWords = len(trainMatrix[0])\n",
    "    pAbusive = sum(trainCategory)/float(numTrainDocs)\n",
    "    p0Num = ones(numWords); p1Num = ones(numWords)      #change to ones() \n",
    "    p0Denom = 2.0; p1Denom = 2.0                        #change to 2.0\n",
    "    for i in range(numTrainDocs):\n",
    "        if trainCategory[i] == 1:        \n",
    "            p1Num += trainMatrix[i]\n",
    "            p1Denom += sum(trainMatrix[i])\n",
    "        else:\n",
    "            p0Num += trainMatrix[i]\n",
    "            p0Denom += sum(trainMatrix[i])\n",
    "    p1Vect = log(p1Num/p1Denom)          #change to log()\n",
    "    p0Vect = log(p0Num/p0Denom)          #change to log()\n",
    "\n",
    "    return p0Vect,p1Vect,pAbusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 分类：根据计算后，哪个类别的概率大，则属于哪个类别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n",
    "    p1 = sum(vec2Classify * p1Vec) + log(pClass1)    #element-wise mult\n",
    "    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6 测试程序\n",
    "\n",
    "加载数据集+提炼词表；\n",
    "\n",
    "训练模型：根据六条训练集计算先验概率和条件概率；\n",
    "\n",
    "测试模型：对训练两条测试文本进行分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testingNB():\n",
    "    listOPosts,listClasses = loadDataSet()\n",
    "    myVocabList = createVocabList(listOPosts)\n",
    "    trainMat=[]\n",
    "    for postinDoc in listOPosts:\n",
    "        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n",
    "    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))\n",
    "    testEntry = ['love', 'my', 'dalmation']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print( testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))\n",
    "    testEntry = ['stupid', 'garbage']\n",
    "    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n",
    "    print( testEntry,'classified as: ',classifyNB(thisDoc,p0V,p1V,pAb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love', 'my', 'dalmation'] classified as:  0\n",
      "['stupid', 'garbage'] classified as:  1\n"
     ]
    }
   ],
   "source": [
    "testingNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 对邮件的文本划分成词汇，长度小于2的默认为不是词汇，过滤掉即可。返回一串小写的拆分后的邮件信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textParse(bigString):\n",
    "    import re\n",
    "    listOfTokens = re.split(r'\\W*', bigString)\n",
    "    return [tok.lower() for tok in listOfTokens if len(tok) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 文档词袋模型：使用数组代替集合数据结构，可以保存词汇频率信息。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bagOfWords2VecMN(vocabList,inputSet):\n",
    "    returnVec = [0] * len(vocabList)\n",
    "    for word in inputSet:\n",
    "        if word in vocabList:\n",
    "            returnVec[vocabList.index[word]] += 1\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 输入为25封正常邮件和25封垃圾邮件。50封邮件中随机选取10封作为测试样本，剩余40封作为训练样本。\n",
    "\n",
    "　　　训练模型：40封训练样本，训练出先验概率和条件概率；\n",
    "\n",
    "　　　测试模型：遍历10个测试样本，计算垃圾邮件分类的正确率。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spamTest():\n",
    "    docList=[]; classList = []; fullText =[]\n",
    "    for i in range(1,26):\n",
    "        wordList = textParse(open('email/spam/%d.txt' % i).read())\n",
    "        # print wordList\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(1)\n",
    "        wordList = textParse(open('email/ham/%d.txt' % i).read())\n",
    "        docList.append(wordList)\n",
    "        fullText.extend(wordList)\n",
    "        classList.append(0)\n",
    "    vocabList = createVocabList(docList)#create vocabulary\n",
    "    trainingSet = range(50); testSet=[]           #create test set\n",
    "    for i in range(10):\n",
    "        randIndex = int(random.uniform(0,len(trainingSet)))\n",
    "        testSet.append(trainingSet[randIndex])\n",
    "        del(trainingSet[randIndex])  \n",
    "    trainMat=[]; trainClasses = []\n",
    "    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n",
    "        trainMat.append(bagOfWords2VecMN(vocabList, docList[docIndex]))\n",
    "        trainClasses.append(classList[docIndex])\n",
    "    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n",
    "    errorCount = 0\n",
    "    for docIndex in testSet:        #classify the remaining items\n",
    "        wordVector = bagOfWords2VecMN(vocabList, docList[docIndex])\n",
    "        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n",
    "            errorCount += 1\n",
    "            print( \"classification error\",docList[docIndex])\n",
    "    print( 'the error rate is: ',float(errorCount)/len(testSet))\n",
    "    #return vocabList,fullText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2，SVM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "应用场景：文本分类、图像识别、主要二分类领域\n",
    "SVM优点\n",
    "1、解决小样本下机器学习问题。\n",
    "2、解决非线性问题。\n",
    "3、无局部极小值问题。（相对于神经网络等算法）\n",
    "4、可以很好的处理高维数据集。\n",
    "5、泛化能力比较强。\n",
    "\n",
    "SVM缺点\n",
    "1、对于核函数的高维映射解释力不强，尤其是径向基函数。\n",
    "2、对缺失数据敏感"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM的原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 利用SVM模型进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import SVC\n",
    "import sys\n",
    " \n",
    " \n",
    "def load_file_and_preprocessing():\n",
    "    neg = pd.read_excel('data/neg.xls', header=None, index=None)\n",
    "    pos = pd.read_excel('data/pos.xls', header=None, index=None)\n",
    "    \n",
    "    cw = lambda x: list(jieba.cut(x))\n",
    "    pos['words'] = pos[0].apply(cw)\n",
    "    neg['words'] = neg[0].apply(cw)\n",
    "    \n",
    "    # print pos['words']\n",
    "    # use 1 for positive sentiment, 0 for negative\n",
    "    y = np.concatenate((np.ones(len(pos)), np.zeros(len(neg))))\n",
    "    \n",
    "    # 切割训练集和测试集\n",
    "    x_train, x_test, y_train, y_test = train_test_split(np.concatenate((pos['words'], neg['words'])), y, test_size=0.2)\n",
    "    \n",
    "    np.save('data/y_train.npy', y_train)\n",
    "    np.save('data/y_test.npy', y_test)\n",
    "    return x_train, x_test\n",
    " \n",
    " \n",
    "def build_sentence_vector(text, size, imdb_w2v):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    " \n",
    " \n",
    "# 计算词向量\n",
    "def get_train_vecs(x_train, x_test):\n",
    "    n_dim = 300\n",
    "    # 初始化模型和词表\n",
    "    imdb_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "    imdb_w2v.build_vocab(x_train)\n",
    "    \n",
    "    # 在评论训练集上建模(可能会花费几分钟)\n",
    "    imdb_w2v.train(x_train)\n",
    "    \n",
    "    train_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_train])\n",
    "    # train_vecs = scale(train_vecs)\n",
    "    \n",
    "    np.save('data/train_vecs.npy', train_vecs)\n",
    "    print(train_vecs.shape)\n",
    "    # 在测试集上训练\n",
    "    imdb_w2v.train(x_test)\n",
    "    imdb_w2v.save('data/w2v_model.pkl')\n",
    "    # Build test tweet vectors then scale\n",
    "    test_vecs = np.concatenate([build_sentence_vector(z, n_dim, imdb_w2v) for z in x_test])\n",
    "    # test_vecs = scale(test_vecs)\n",
    "    np.save('data/test_vecs.npy', test_vecs)\n",
    "    print(test_vecs.shape)\n",
    " \n",
    " \n",
    "def get_data():\n",
    "    train_vecs = np.load('data/train_vecs.npy')\n",
    "    y_train = np.load('data/y_train.npy')\n",
    "    test_vecs = np.load('data/test_vecs.npy')\n",
    "    y_test = np.load('data/y_test.npy')\n",
    "    return train_vecs, y_train, test_vecs, y_test\n",
    " \n",
    " \n",
    "def svm_train(train_vecs, y_train, test_vecs, y_test):\n",
    "    clf = SVC(kernel='rbf', verbose=True)\n",
    "    clf.fit(train_vecs, y_train)\n",
    "    joblib.dump(clf, 'data/svm_model.pkl')\n",
    "    print(clf.score(test_vecs, y_test))\n",
    " \n",
    " \n",
    "def get_predict_vecs(words):\n",
    "    n_dim = 300\n",
    "    imdb_w2v = Word2Vec.load('data/w2v_model.pkl')\n",
    "    # imdb_w2v.train(words)\n",
    "    train_vecs = build_sentence_vector(words, n_dim, imdb_w2v)\n",
    "    # print train_vecs.shape\n",
    "    return train_vecs\n",
    " \n",
    " \n",
    "def svm_predict(string):\n",
    "    words = jieba.lcut(string)\n",
    "    words_vecs = get_predict_vecs(words)\n",
    "    clf = joblib.load('data/svm_model.pkl')\n",
    "    \n",
    "    result = clf.predict(words_vecs)\n",
    "    \n",
    "    if int(result[0]) == 1:\n",
    "        print(string, ' positive')\n",
    "    else:\n",
    "        print(string, ' negative')\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    ##对输入句子情感进行判断\n",
    "    string = '电池充完了电连手机都打不开.简直烂的要命.真是金玉其外,败絮其中!连5号电池都不如'\n",
    "    # string='牛逼的手机，从3米高的地方摔下去都没坏，质量非常好'\n",
    "    svm_predict(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3，LDA主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDA模型生成过程可描述如下：\n",
    "\n",
    "    （１）文档ｄ中词项总数Ｎｄ服从泊松分布，其参数为ξ：Ｎｄ～Poisson（ξ）\n",
    "\n",
    "    （２）对每篇文档ｄ∈｛１，２，．．．，｜Ｄ｜｝，按概率生成其主题分布：θ→ｄ～ Dirichlet（α→）；\n",
    "\n",
    "    （３）对每个主题ｚ∈｛１，２，．．．，Ｋ｝，按概率生成其词项分布：φ→ｋ～ Dirichlet（β→）；\n",
    "\n",
    "    （４）对文档ｄ中每个词wn的生成过程，其中ｎ∈｛１，２，．．．，Ｎｄ｝，有：         \n",
    "\n",
    "        １）根据主题分布θ→ｄ生成文档ｄ词项ｗｎ主题：ｚｄ，ｎ～Multionmial（θ→ｄ）；         \n",
    "\n",
    "        ２）根 据 词 项 分 布φｚｄ，ｎ→生 成 所 选 主 题 词 项：ｗｄ，ｎ～Multionmial（φ→ｚｄ，ｎ）。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pLSA、共轭先验分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "其具体步骤总结如下：\n",
    "\n",
    "输入：LDA模型语料库、KNN分类语料库\n",
    "\n",
    "输出：待分类文本的分类结果\n",
    "\n",
    "（１）通过文本语料库训练LDA模型并推断KNN训练和测试文本集的主题分布；\n",
    "\n",
    "（２）选取特征词并修改主题分布；\n",
    "\n",
    "（３）根据下式计算主题相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src='4.webp'>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src='4.webp'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用LDA生成主题特征，在之前特征的基础上加入主题特征进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA数学八卦 lda2 合并特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
