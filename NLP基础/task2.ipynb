{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 基本文本处理技能\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 分词的概念（分词的正向最大、逆向最大、双向最大匹配法）；\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 正向最大\n",
    "    正向即从前往后取词，从7->1，每次减一个字，直到词典命中或剩下1个单字。\n",
    "逆向最大\n",
    "    逆向即从后往前取词，其他逻辑和正向相同。\n",
    "双向最大匹配法\n",
    "    正向最大匹配法和逆向最大匹配法，都有其局限性，我举得例子是正向最大匹配法局限性的例子，逆向也同样存在（如：长春药店，逆向切分为“长/春药店”），因此有人又提出了双向最大匹配法，双向最大匹配法。即，两种算法都切一遍，然后根据大颗粒度词越多越好，非词典词和单字词越少越好的原则，选取其中一种分词结果输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 词、字符频率统计；（可以使用Python中的collections.Counter模块，也可以自己寻找其他好用的库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding=utf-8\n",
    "import jieba\n",
    "import jieba.analyse\n",
    "txt = open('Chinese.txt',\"r\")\n",
    "seg_txt=[]\n",
    "for line in txt:\n",
    "    #第一行是关键词提取。\n",
    "    #第二行是分词提取\n",
    "    #第三行是关键词提取（与第一行算法有差异）\n",
    "    seg_list = jieba.analyse.extract_tags(line.strip('\\n\\r\\t'))\n",
    "    #seg_list = jieba.lcut(line.strip('\\n\\r\\t'))\n",
    "    #seg_list = jieba.analyse.textrank(line.strip('\\n\\r\\t'))\n",
    "    seg_txt.extend(seg_list)\n",
    "#至此所有的中文词以list的形式存到了seg_txt中。  \n",
    "\n",
    "#下面进行词频排序，由高到底。 \n",
    "word_dict={}\n",
    "for item in seg_txt:\n",
    "            if item not in word_dict:\n",
    "                word_dict[item] = 1\n",
    "            else:\n",
    "                word_dict[item] += 1\n",
    "\n",
    "number=list(word_dict.items())\n",
    "number.sort(key=lambda x:x[1], reverse=True)\n",
    "i=0\n",
    "while i<100:\n",
    "    print number[i][0],number[i][1]\n",
    "    i=i+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "s=\"我 是 中国人 我 热爱 中国\"\n",
    "s=s.split()\n",
    "t=list(Counter(s).items())\n",
    "tt=sorted(t,key=lambda x:x[1],reverse=True)\n",
    "print(t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 语言模型中unigram、bigram、trigram的概念；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram\n",
    "     一元分词，把句子分成一个一个的汉字\n",
    "bigram\n",
    "    二元分词，把句子从头到尾每两个字组成一个词语\n",
    "trigram\n",
    "    三元分词，把句子从头到尾每三个字组成一个词语."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 unigram、bigram频率统计；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://mp.weixin.qq.com/s/iN2UFrp_bue9D-oULSf3Lg\n",
    "import collections\n",
    "text = u\"\"\"5日下午，中共中央总书记、国家主席、中央军委主席习近平参加他所在的十三届全国人大二次会议内蒙古代表团审议\"\"\"\n",
    "print(\"-------------counter unigram--------------\")\n",
    "unigram_counter = collections.Counter([text[i] for i in range(0,len(text))])\n",
    "for k,v in unigram_counter.items():\n",
    "    print(k,v)\n",
    "\n",
    "print(\"-------------counter bigram--------------\")\n",
    "bigram_counter = collections.Counter([(text[i],text[i+1]) for i in range(0,len(text)-1)])\n",
    "for k,v in bigram_counter.items():\n",
    "    print(k,v)\n",
    "\n",
    "print(\"-------------counter bigram--------------\")\n",
    "bigram_counter = collections.Counter([(text[i],text[i+1],text[i+2]) for i in range(0,len(text)-2)])\n",
    "for k,v in bigram_counter.items():\n",
    "    print(k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    " \n",
    "class NGram(object):\n",
    " \n",
    "    def __init__(self, n):\n",
    "        # n is the order of n-gram language model\n",
    "        self.n = n\n",
    "        self.unigram = {}\n",
    "        self.bigram = {}\n",
    " \n",
    "    # scan a sentence, extract the ngram and update their\n",
    "    # frequence.\n",
    "    #\n",
    "    # @param    sentence    list{str}\n",
    "    # @return   none\n",
    "    def scan(self, sentence):\n",
    "        # file your code here\n",
    "        for line in sentence:\n",
    "            self.ngram(line.split())\n",
    "        #unigram\n",
    "        if self.n == 1:\n",
    "            try:\n",
    "                fip = open(\"data.uni\",\"w\")\n",
    "            except:\n",
    "                print >> sys.stderr ,\"failed to open data.uni\"\n",
    "            for i in self.unigram:\n",
    "                fip.write(\"%s %d\\n\" % (i,self.unigram[i]))\n",
    "        if self.n == 2:\n",
    "            try:\n",
    "                fip = open(\"data.bi\",\"w\")\n",
    "            except:\n",
    "                print >> sys.stderr ,\"failed to open data.bi\"\n",
    "            for i in self.bigram:\n",
    "                fip.write(\"%s %d\\n\" % (i,self.bigram[i]))\n",
    "    # caluclate the ngram of the words\n",
    "    #\n",
    "    # @param    words       list{str}\n",
    "    # @return   none\n",
    "    def ngram(self, words):\n",
    "        # unigram\n",
    "        if self.n == 1:\n",
    "            for word in words:\n",
    "                if word not in self.unigram:\n",
    "                    self.unigram[word] = 1\n",
    "                else:\n",
    "                    self.unigram[word] = self.unigram[word] + 1\n",
    " \n",
    "        # bigram\n",
    "        if self.n == 2:\n",
    "            num = 0\n",
    "            stri = ''\n",
    "            for i in words:\n",
    "                num = num + 1\n",
    "                if num == 2:\n",
    "                    stri  = stri + \" \"\n",
    "                stri = stri + i\n",
    "                if num == 2:\n",
    "                    if stri not in self.bigram:\n",
    "                        self.bigram[stri] = 1\n",
    "                    else:\n",
    "                        self.bigram[stri] = self.bigram[stri] + 1\n",
    "                    num = 0\n",
    "                    stri = ''\n",
    " \n",
    "if __name__==\"__main__\":\n",
    "    import sys\n",
    "    try:\n",
    "        fip = open(sys.argv[1],\"r\")\n",
    "    except:\n",
    "        print >> sys.stderr, \"failed to open input file\"\n",
    "    sentence = []\n",
    "    for line in fip:\n",
    "        if len(line.strip())!=0:\n",
    "            sentence.append(line.strip())\n",
    "    uni = NGram(1)\n",
    "    bi = NGram(2)\n",
    "    uni.scan(sentence)\n",
    "    bi.scan(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 文本矩阵化：要求采用词袋模型且是词级别的矩阵化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 分词\n",
    "\n",
    "3.2 去停用词；构造词表。\n",
    "\n",
    "3.3 每篇文档的向量化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://blog.csdn.net/weixin_41781408/article/details/88146030\n",
    "\n",
    "import jieba\n",
    "import re\n",
    "stopwords = {}\n",
    "fstop = open('stop_words.txt', 'r',encoding='utf-8',errors='ingnore')\n",
    "for eachWord in fstop:\n",
    "    stopwords[eachWord.strip()] = eachWord.strip()  #停用词典\n",
    "fstop.close()\n",
    "f1=open('tt1.txt','r',encoding='utf-8',errors='ignore')\n",
    "f2=open('fenci.txt','w',encoding='utf-8')\n",
    "\n",
    "line=f1.readline()\n",
    "while line:\n",
    "    line = line.strip()  #去前后的空格\n",
    "    line = re.sub(r\"[0-9\\s+\\.\\!\\/_,$%^*()?;；:-【】+\\\"\\']+|[+——！，;:。？、~@#￥%……&*（）]+\", \" \", line) #去标点符号\n",
    "    seg_list=jieba.cut(line,cut_all=False)  #结巴分词\n",
    "    outStr=\"\"\n",
    "    for word in seg_list:\n",
    "        if word not in stopwords:\n",
    "            outStr+=word\n",
    "            outStr+=\" \"\n",
    "    f2.write(outStr)\n",
    "    line=f1.readline()\n",
    "f1.close()\n",
    "f2.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 向量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "f=open(\"fenci.txt\",\"r\",encoding=\"utf-8\")\n",
    "t=f.read()\n",
    "t=set(t.split())\n",
    "# print(t)\n",
    "corpus_dict=dict(zip(t,range(len(t))))\n",
    "print(corpus_dict)\n",
    "# 建立句子的向量表示\n",
    "text='''韦斯特一心瞄准自由市场 怎奈伤病挡路 钱途渺茫'''\n",
    "text1 = list(jieba.cut(text, cut_all=False))\n",
    "print(text1)\n",
    "def vector_rep(text, corpus_dict):\n",
    "    vec,vec2 = [],[]\n",
    "    for key in corpus_dict.keys():\n",
    "        if key in text:\n",
    "            vec.append((corpus_dict[key], text.count(key)))\n",
    "            vec2.append(text.count(key))\n",
    "        else:\n",
    "            vec.append((corpus_dict[key], 0))\n",
    "            vec2.append(0)\n",
    "    vec = sorted(vec, key= lambda x: x[0])\n",
    "    return vec,vec2\n",
    "vec1,vec2 = vector_rep(text1, corpus_dict)\n",
    "print(vec1)\n",
    "print(vec2)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.2 .采用Word2Vec处理结巴分词的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import logging\n",
    "import os.path\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    program=os.path.basename(sys.argv[0])\n",
    "    logger=logging.getLogger(program)\n",
    "    logging.basicConfig(format=\"%(asctime)s:%(levelname)s:%(message)s\")\n",
    "    logger.info(\"running %s \"% \" \".join(sys.argv))\n",
    "\n",
    "    if len(sys.argv)<4:\n",
    "        print(globals()['__doc__']%local())\n",
    "        sys.exit()\n",
    "    inp,outp1,outp2=sys.argv[1:4]\n",
    "\n",
    "    model=Word2Vec(LineSentence(inp),size=400,window=5,min_count=5,workers=multiprocessing.cpu_count())\n",
    "    model.save(outp1)\n",
    "    model.wv.save_word2vec_format(outp2,binary=False)\n",
    "\n",
    "     #  cmd python day3word2vec.py fenci.txt 2800a.model 2800a.vector 命令行中运行\n",
    "\n",
    "        #处理好的模型的词嵌入向量。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
