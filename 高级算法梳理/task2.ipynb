{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://blog.csdn.net/weixin_39982211/article/details/89048783  作业范例\n",
    "https://blog.csdn.net/beautiful_well/article/details/98884622"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 前向分布算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本思想：每次只学习一个基函数及系数，逐步逼近最优解。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 负梯度拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 顾名思义，就是在损失函数梯度的负方向上进行学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对于分类算法，其损失函数一般有对数损失函数和指数损失函数两种:\n",
    "\n",
    "　　　　a) 如果是指数损失函数，则损失函数表达式为\n",
    "\n",
    "L(y,f(x))=exp(−yf(x))L(y,f(x))=exp(−yf(x))\n",
    "\n",
    "　　　　其负梯度计算和叶子节点的最佳负梯度拟合参见Adaboost原理篇。\n",
    "\n",
    "　　　　b) 如果是对数损失函数，分为二元分类和多元分类两种，参见4.1节和4.2节。\n",
    "对于回归算法，常用损失函数有如下4种:\n",
    "\n",
    "　　　　a)均方差，这个是最常见的回归损失函数了\n",
    "\n",
    "L(y,f(x))=(y−f(x))2L(y,f(x))=(y−f(x))2\n",
    "\n",
    "　　　　b)绝对损失，这个损失函数也很常见\n",
    "\n",
    "L(y,f(x))=|y−f(x)|L(y,f(x))=|y−f(x)|\n",
    "\n",
    "　　　　　　对应负梯度误差为：\n",
    "\n",
    "sign(yi−f(xi))sign(yi−f(xi))\n",
    "\n",
    "　　　　c)Huber损失，它是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心附近的点采用均方差。这个界限一般用分位数点度量。    \n",
    "       \n",
    "        d) 分位数损失。它对应的是分位数回归的损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入是训练集样本T={(x,y1),(x2,y2),...(xm,ym)}T={(x,y1),(x2,y2),...(xm,ym)}， 最大迭代次数T, 损失函数L。\n",
    "\n",
    "　　　　输出是强学习器f(x)\n",
    "\n",
    "　　　　1) 初始化弱学习器\n",
    "\n",
    "f0(x)=argmin⏟c∑i=1mL(yi,c)f0(x)=argmin⏟c∑i=1mL(yi,c)\n",
    "\n",
    "　　　　2) 对迭代轮数t=1,2,...T有：\n",
    "\n",
    "　　　　　　a)对样本i=1,2，...m，计算负梯度\n",
    "\n",
    "rti=−[∂L(yi,f(xi)))∂f(xi)]f(x)=ft−1(x)rti=−[∂L(yi,f(xi)))∂f(xi)]f(x)=ft−1(x)\n",
    "\n",
    "　　　　　　b)利用(xi,rti)(i=1,2,..m)(xi,rti)(i=1,2,..m), 拟合一颗CART回归树,得到第t颗回归树，其对应的叶子节点区域为Rtj,j=1,2,...,JRtj,j=1,2,...,J。其中J为回归树t的叶子节点的个数。\n",
    "\n",
    "　　　　　　c) 对叶子区域j =1,2,..J,计算最佳拟合值\n",
    "\n",
    "ctj=argmin⏟c∑xi∈RtjL(yi,ft−1(xi)+c)ctj=argmin⏟c∑xi∈RtjL(yi,ft−1(xi)+c)\n",
    "\n",
    "　　　　　　d) 更新强学习器\n",
    "\n",
    "ft(x)=ft−1(x)+∑j=1JctjI(x∈Rtj)ft(x)=ft−1(x)+∑j=1JctjI(x∈Rtj)\n",
    "\n",
    "　　　　3) 得到强学习器f(x)的表达式\n",
    "\n",
    "f(x)=fT(x)=f0(x)+∑t=1T∑j=1JctjI(x∈Rtj)f(x)=fT(x)=f0(x)+∑t=1T∑j=1JctjI(x∈Rtj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 二分类，多分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT的分类算法从思想上和GBDT的回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。\n",
    "\n",
    "为了解决这个问题，主要有两个方法，一个是用指数损失函数，此时GBDT退化为Adaboost算法。另一种方法是用类似于逻辑回归的对数似然损失函数的方法。也就是说，我们用的是类别的预测概率值和真实概率值的差来拟合损失。本文仅讨论用对数似然损失函数的GBDT分类。而对于对数似然损失函数，我们又有二元分类和多元分类的区别。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二元GBDT分类算法\n",
    "对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数为：\n",
    "\n",
    "L(y,f(x))=log(1+exp(−yf(x)))L(y,f(x))=log(1+exp(−yf(x)))\n",
    "\n",
    "　　　　其中y∈{−1,+1}y∈{−1,+1}。则此时的负梯度误差为\n",
    "\n",
    "rti=−[∂L(y,f(xi)))∂f(xi)]f(x)=ft−1(x)=yi/(1+exp(yif(xi)))rti=−[∂L(y,f(xi)))∂f(xi)]f(x)=ft−1(x)=yi/(1+exp(yif(xi)))\n",
    "\n",
    "　　　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为\n",
    "\n",
    "ctj=argmin⏟c∑xi∈Rtjlog(1+exp(−yi(ft−1(xi)+c)))ctj=argmin⏟c∑xi∈Rtjlog(1+exp(−yi(ft−1(xi)+c)))\n",
    "\n",
    "　　　　由于上式比较难优化，我们一般使用近似值代替\n",
    "\n",
    "ctj=∑xi∈Rtjrti/∑xi∈Rtj|rti|(1−|rti|)ctj=∑xi∈Rtjrti/∑xi∈Rtj|rti|(1−|rti|)\n",
    "\n",
    "　　　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，二元GBDT分类和GBDT回归算法过程相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多元GBDT分类算法\n",
    "多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假设类别数为K，则此时我们的对数似然损失函数为：\n",
    "\n",
    "L(y,f(x))=−∑k=1Kyklogpk(x)L(y,f(x))=−∑k=1Kyklogpk(x)\n",
    "\n",
    "　　　　其中如果样本输出类别为k，则yk=1yk=1。第k类的概率pk(x)pk(x)的表达式为：\n",
    "\n",
    "pk(x)=exp(fk(x))/∑l=1Kexp(fl(x))pk(x)=exp(fk(x))/∑l=1Kexp(fl(x))\n",
    "\n",
    "　　　　集合上两式，我们可以计算出第tt轮的第ii个样本对应类别ll的负梯度误差为\n",
    "\n",
    "rtil=−[∂L(yi,f(xi)))∂f(xi)]fk(x)=fl,t−1(x)=yil−pl,t−1(xi)rtil=−[∂L(yi,f(xi)))∂f(xi)]fk(x)=fl,t−1(x)=yil−pl,t−1(xi)\n",
    "\n",
    "　　　　观察上式可以看出，其实这里的误差就是样本ii对应类别ll的真实概率和t−1t−1轮预测概率的差值。\n",
    "\n",
    "　　　　对于生成的决策树，我们各个叶子节点的最佳负梯度拟合值为\n",
    "\n",
    "ctjl=argmin⏟cjl∑i=0m∑k=1KL(yk,ft−1,l(x)+∑j=0JcjlI(xi∈Rtj))ctjl=argmin⏟cjl∑i=0m∑k=1KL(yk,ft−1,l(x)+∑j=0JcjlI(xi∈Rtj))\n",
    "\n",
    "　　　　由于上式比较难优化，我们一般使用近似值代替\n",
    "\n",
    "ctjl=K−1K∑xi∈Rtjlrtil∑xi∈Rtil|rtil|(1−|rtil|)ctjl=K−1K∑xi∈Rtjlrtil∑xi∈Rtil|rtil|(1−|rtil|)\n",
    "\n",
    "　　　　除了负梯度计算和叶子节点的最佳负梯度拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 正则化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 和Adaboost一样，我们也需要对GBDT进行正则化，防止过拟合。GBDT的正则化主要有三种方式。\n",
    "\n",
    "　　　　第一种是和Adaboost类似的正则化项，即步长(learning rate)。定义为νν,对于前面的弱学习器的迭代\n",
    "\n",
    "fk(x)=fk−1(x)+hk(x)fk(x)=fk−1(x)+hk(x)\n",
    "\n",
    "　　　　如果我们加上了正则化项，则有\n",
    "\n",
    "fk(x)=fk−1(x)+νhk(x)fk(x)=fk−1(x)+νhk(x)\n",
    "\n",
    "　　　　νν的取值范围为0<ν≤10<ν≤1。对于同样的训练集学习效果，较小的νν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。\n",
    "\n",
    " \n",
    "\n",
    "　　　　第二种正则化的方式是通过子采样比例（subsample）。取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间。\n",
    "                                     \n",
    "        第三种是对于弱学习器即CART回归树进行正则化剪枝。在决策树原理篇里我们已经讲过，这里就不重复了。                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 优缺点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT主要的优点有：\n",
    "\n",
    "　　　　1) 可以灵活处理各种类型的数据，包括连续值和离散值。\n",
    "\n",
    "　　　　2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。\n",
    "\n",
    "　　　　3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。\n",
    "\n",
    "　　　　GBDT的主要缺点有：\n",
    "\n",
    "　　　　1)由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 sklearn参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sklearn.ensemble.GradientBoostingRegressor(\n",
    "\n",
    "loss='ls',      ##默认ls损失函数'ls'是指最小二乘回归lad'（最小绝对偏差）'huber'是两者的组合\n",
    "\n",
    "n_estimators=100, ##默认100 回归树个数 弱学习器个数\n",
    "\n",
    "learning_rate=0.1,  ##默认0.1学习速率/步长0.0-1.0的超参数  每个树学习前一个树的残差的步长\n",
    "\n",
    "max_depth=3,   ## 默认值为3每个回归树的深度  控制树的大小 也可用叶节点的数量max leaf nodes控制\n",
    "\n",
    "subsample=1,  ##用于拟合个别基础学习器的样本分数 选择子样本<1.0导致方差的减少和偏差的增加\n",
    "\n",
    "min_samples_split=2, ##生成子节点所需的最小样本数 如果是浮点数代表是百分比\n",
    "\n",
    "min_samples_leaf=1, ##叶节点所需的最小样本数  如果是浮点数代表是百分比\n",
    "\n",
    "max_features=None, ##在寻找最佳分割点要考虑的特征数量auto全选/sqrt开方/log2对数/None全选/int自定义几个/float百分比\n",
    "\n",
    "max_leaf_nodes=None, ##叶节点的数量 None不限数量\n",
    "\n",
    "min_impurity_split=1e-7, ##停止分裂叶子节点的阈值\n",
    "\n",
    "verbose=0,  ##打印输出 大于1打印每棵树的进度和性能\n",
    "\n",
    "warm_start=False, ##True在前面基础上增量训练 False默认擦除重新训练 增加树\n",
    "\n",
    "random_state=0  ##随机种子-方便重现\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 应用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GBDT几乎可用于所有回归问题（线性/非线性），相对logistic regression仅能用于线性回归，GBDT的适用面非常广。亦可用于二分类问题（设定阈值，大于阈值为正例，反之为负例）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
